{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Access Twitter API"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter  #pip install twitter\n",
      "\n",
      "def oauth_login():\n",
      "    \n",
      "    consumer_key = '0dmtxDXylK1IWLDzsT5aixvsy'\n",
      "    consumer_secret = '9IIJ9a6G56hLFoSEZRwM7yUzNSTcc79kPBqO2bisBOqnr80Kk2'\n",
      "    access_token = '2790391620-e0Dc9f5UHKx5PKShiedZwOxnh4i3eeUs3AqdNme'\n",
      "    access_token_secret = 'Ji7tqNNsZB89nUBhgIsLapzVL4edr3LCHnn37aXcQPCcX'\n",
      "    \n",
      "    \n",
      "    auth = twitter.oauth.OAuth(access_token, access_token_secret,\n",
      "                               consumer_key, consumer_secret)\n",
      "    \n",
      "    twitter_api = twitter.Twitter(auth=auth)\n",
      "    return twitter_api\n",
      "\n",
      "twitter_api = oauth_login()    \n",
      "print twitter_api"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<twitter.api.Twitter object at 0x000000000414A940>\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Robust Request (Avoiding Rate Limit and Other Errors) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import time\n",
      "from urllib2 import URLError\n",
      "from httplib import BadStatusLine\n",
      "import json\n",
      "import twitter\n",
      "\n",
      "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
      "\n",
      "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
      "    \n",
      "        if wait_period > 3600: # Seconds\n",
      "            print >> sys.stderr, 'Too many retries. Quitting.'\n",
      "            raise e\n",
      "\n",
      "    \n",
      "        if e.e.code == 401:\n",
      "            print >> sys.stderr, '401 Error (Not Authorized)'\n",
      "            return None\n",
      "        elif e.e.code == 404:\n",
      "            print >> sys.stderr, '404 Error (Not Found)'\n",
      "            return None\n",
      "        elif e.e.code == 429: \n",
      "            print >> sys.stderr, '429 Error (Rate Limit Exceeded)'\n",
      "            if sleep_when_rate_limited:\n",
      "                print >> sys.stderr, \"Retry in 15 minutes\"\n",
      "                sys.stderr.flush()\n",
      "                time.sleep(60*15 + 5)\n",
      "                print >> sys.stderr, 'End of 15 mins.... Trying again.'\n",
      "                return 2\n",
      "            else:\n",
      "                raise e \n",
      "        elif e.e.code in (500, 502, 503, 504):\n",
      "            print >> sys.stderr, 'Encountered %i Error. Retrying in %i seconds' % \\\n",
      "                (e.e.code, wait_period)\n",
      "            time.sleep(wait_period)\n",
      "            wait_period *= 1.5\n",
      "            return wait_period\n",
      "        else:\n",
      "            raise e\n",
      "    \n",
      "    wait_period = 2 \n",
      "    error_count = 0 \n",
      "\n",
      "    while True:\n",
      "        try:\n",
      "            return twitter_api_func(*args, **kw)\n",
      "        except twitter.api.TwitterHTTPError, e:\n",
      "            error_count = 0 \n",
      "            wait_period = handle_twitter_http_error(e, wait_period)\n",
      "            if wait_period is None:\n",
      "                return\n",
      "        except URLError, e:\n",
      "            error_count += 1\n",
      "            print >> sys.stderr, \"URLError encountered. Continuing.\"\n",
      "            if error_count > max_errors:\n",
      "                print >> sys.stderr, \"Too many consecutive errors\"\n",
      "                raise\n",
      "        except BadStatusLine, e:\n",
      "            error_count += 1\n",
      "            print >> sys.stderr, \"BadStatusLine encountered. Continuing.\"\n",
      "            if error_count > max_errors:\n",
      "                print >> sys.stderr, \"Too many consecutive errors\"\n",
      "                raise\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extracting Followers and Finding Common Followers (Republicans)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from functools import partial\n",
      "from sys import maxint\n",
      "\n",
      "def get_followers_ids(twitter_api, screen_name=None, user_id=None, followers_limit=maxint):\n",
      "\n",
      "    assert (screen_name != None) != (user_id != None), \\\n",
      "    \"Must have screen_name or user_id, but not both\"\n",
      "\n",
      "    \n",
      "    get_followers_ids = partial(make_twitter_request, twitter_api.followers.ids, \n",
      "                                count=100)\n",
      "\n",
      "    followers_ids = []\n",
      "    \n",
      "    for twitter_api_func, limit, ids, label in [\n",
      "                    [get_followers_ids, followers_limit, followers_ids, \"followers\"]\n",
      "                ]:\n",
      "        \n",
      "        if limit == 0: continue\n",
      "        \n",
      "        cursor = -1\n",
      "        while cursor != 0:\n",
      "        \n",
      "            if screen_name: \n",
      "                response = twitter_api_func(screen_name=screen_name, cursor=cursor)\n",
      "            else: \n",
      "                response = twitter_api_func(user_id=user_id, cursor=cursor)\n",
      "\n",
      "            if response is not None:\n",
      "                ids += response['ids']\n",
      "                cursor = response['next_cursor']\n",
      "        \n",
      "            print >> sys.stderr, 'Fetched {0} total {1} ids for {2}'.format(len(ids), \n",
      "                                                    label, (user_id or screen_name))\n",
      "        \n",
      "\n",
      "            if len(ids) >= limit or response is None:\n",
      "                break\n",
      "\n",
      "    return followers_ids[:followers_limit]\n",
      "\n",
      "twitter_api = oauth_login()\n",
      "\n",
      "\n",
      "followers_id1 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"BarackObama\",  \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id2 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"elizabethforma\", \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id3 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"JoeBiden\", \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id4 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"davidaxelrod\",  \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id5 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"CoryBooker\", \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id6 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"StephenAtHome\",  \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id7 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"TheFix\",  \n",
      "                                  followers_limit=100)\n",
      "\n",
      "followers_id8 = get_followers_ids(twitter_api, \n",
      "                                  screen_name=\"ariannahuff\", \n",
      "                                  followers_limit=100)\n",
      "\n",
      "# followers_id9 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"billmaher\", \n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id10 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"TheDailyShow\",\n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id11 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"KeithOlbermann\", \n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id12 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"maddow\",  \n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id13 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"hillaryclinton\", \n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id14 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"ggreenwald\", \n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id15 = get_followers_ids(twitter_api, \n",
      "#                                   screen_name=\"chrislhayes\", \n",
      "#                                   followers_limit=100)\n",
      "\n",
      "# followers_id = (followers_id1 + followers_id2 + followers_id3 + followers_id4 + followers_id5 + \n",
      "#                 followers_id6 + followers_id7 + followers_id8 + followers_id9 + followers_id10 + \n",
      "#                 followers_id11 + followers_id12 + followers_id13 + followers_id14 + followers_id15)\n",
      "\n",
      "\n",
      "followers_id = (followers_id1 + followers_id2 + followers_id3 + followers_id4 + followers_id5 + \n",
      "                followers_id6 + followers_id7 + followers_id8)\n",
      "        \n",
      "c = [[x,followers_id.count(x)] for x in set(followers_id)]\n",
      "followers = [x[0] for x in c if x[1] > 3]\n",
      "\n",
      "\n",
      "    \n",
      "print \"Total number of Common followers: {}\".format(len(followers))\n",
      "print '\\n'.join(map(str,followers))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Fetched 100 total followers ids for BarackObama\n",
        "Fetched 100 total followers ids for elizabethforma"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 100 total followers ids for JoeBiden"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 100 total followers ids for davidaxelrod"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 100 total followers ids for CoryBooker"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 100 total followers ids for StephenAtHome"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 100 total followers ids for TheFix"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 100 total followers ids for ariannahuff"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of Common followers: 3\n",
        "2903175730\n",
        "2903149109\n",
        "2903153715\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fetching Tweets from Common Followers' User Timeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get a user's timeline\n",
      "import re\n",
      "\n",
      "def harvest_user_timeline(twitter_api, screen_name=None, user_id=None, max_results=10):\n",
      "     \n",
      "    assert (screen_name != None) != (user_id != None), \\\n",
      "    \"Must have screen_name or user_id, but not both\"    \n",
      "    \n",
      "    kw = {  # Keyword args for Twitter API\n",
      "        'count': 20,\n",
      "        'trim_user': 'true',\n",
      "        'include_rts' : 'true',\n",
      "        'since_id' : 1\n",
      "        }\n",
      "    \n",
      "    if screen_name:\n",
      "        kw['screen_name'] = screen_name\n",
      "    else:\n",
      "        kw['user_id'] = user_id\n",
      "        \n",
      "    max_pages = 16\n",
      "    results = []\n",
      "    \n",
      "    tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
      "    \n",
      "    if tweets is None: # encountering 401 (Not Authorized)\n",
      "        tweets = []\n",
      "        \n",
      "    results += tweets\n",
      "    \n",
      "    print >> sys.stderr, 'Fetched %i tweets' % len(tweets)\n",
      "    \n",
      "    page_num = 1\n",
      "    \n",
      "    if max_results == kw['count']:\n",
      "        page_num = max_pages # Prevent loop entry\n",
      "    \n",
      "    while page_num < max_pages and len(tweets) > 0 and len(results) < max_results:\n",
      "\n",
      "        kw['max_id'] = min([ tweet['id'] for tweet in tweets]) - 1 \n",
      "    \n",
      "        tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
      "        results += tweets\n",
      "\n",
      "        print >> sys.stderr, 'Fetched %i tweets' % (len(tweets),)\n",
      "    \n",
      "        page_num += 1\n",
      "        \n",
      "    print >> sys.stderr, 'Done fetching tweets'\n",
      "\n",
      "    return results[:max_results]\n",
      "    \n",
      "twitter_api = oauth_login()\n",
      "\n",
      "tweet_list = []\n",
      "for u in followers:\n",
      "    tweets = harvest_user_timeline(twitter_api, user_id=u, \\\n",
      "                                   max_results=10)\n",
      "    tweet_list = tweet_list + [t['text'] for t in tweets]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Fetched 0 tweets\n",
        "Done fetching tweets\n",
        "Fetched 2 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 0 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Done fetching tweets\n",
        "Fetched 2 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Fetched 0 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "\n",
        "Done fetching tweets\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Searching tweets with terms 'ebola' and 'obama' among the fetched tweets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "We filter tweets for searching tweets with the term 'ebola' in them. We also consider tweets using different other terms that divides the two parties' ideology in order to get their sentiment scores as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = re.compile(\".*(ebola|Ebola).*\")\n",
      "\n",
      "# Other terms considered:\n",
      "#     obama|Obama\n",
      "#     obamacare|Obamacare|medicare|Medicare\n",
      "#     immigration|Immigration|immigrants|illegals|mexicans|Mexicans\n",
      "#     military|Military|military spending|Military spending|war|War\n",
      "#     jobs|Jobs|employment|Employment\n",
      "#     gun|guns|gun control|second ammendment|arms|Second ammendment|Second Amendment\n",
      "#     god|lord|God|Lord|Bible|bible|Jesus|jesus|almighty|Almighty\n",
      "#     gay|gays|homosexuals|gay rights|Gay rights|homosexual|Homosexual|Gay|Gays\n",
      "#     conservative|Conservative|Republican|republican|conservatives|republicans\n",
      "#     liberal|liberals|Liberal|Liberals|Democratic|democratic|democrats|democrat\n",
      "\n",
      "\n",
      "target_tweets = [m.group(0) for l in tweet_list for m in [string.search(l)] if m]\n",
      "print target_tweets\n",
      "# f = open('tweet_store_f4.csv', 'w')\n",
      "# f.write(str(target_tweets))\n",
      "# f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Follow me people Twitter keeps saying you have reached your follow limit because the ratio between follow and following is \"unreasonable\"!!!', u'Hey guys this is my new account could not find out my others password or even find it in \"Forgot your password\" so had to make a new one...']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Working with AFINN Wordlist for Sentiment Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download the AFINN lexicon, unzip, and read the latest word list in AFINN-111.txt\n",
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Storing the Sentiment Analysis score in a .csv file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            total += afinn[t]\n",
      "    return total\n",
      "\n",
      "sentiment_list = []\n",
      "for i in target_tweets:\n",
      "    doc = i.split()\n",
      "    sentiment_list = sentiment_list + [afinn_sentiment(doc, afinn)]\n",
      "print sentiment_list\n",
      "# f = csv.writer(open(\"senti_score_f3.csv\", 'wb'))\n",
      "# for senti in sentiment_list:\n",
      "#     f.writerow([senti])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1.0, 0.0]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Removing all scores that are zero (0.0)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ss = [x for x in sentiment_list if x!=0.0]\n",
      "print ss\n",
      "# print(sum(ss)/len(ss))\n",
      "# f = csv.writer(open(\"senti_scoref8_wo_zero.csv\", 'w'))\n",
      "# for senti in ss:\n",
      "#     f.writerow([senti])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1.0]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting the number of words in a tweet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "wc = []\n",
      "for i in target_tweets:\n",
      "    line = i\n",
      "    count = len(re.findall(r'\\w+', line))\n",
      "    wc.append(count)   \n",
      "wc = [i for i in wc]\n",
      "print wc\n",
      "# f = csv.writer(open(\"word_countf2.csv\", 'w'))\n",
      "# for count in wc:\n",
      "#     f.writerow([count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[21, 29]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting frequency of words - alert, concern, cautious in a tweet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq = []\n",
      "for i in target_tweets:\n",
      "    c= (i.count(\"alert\")+i.count(\"concern\")+i.count(\"concerned\")+i.count(\"cautious\")+i.count(\"caution\")) \n",
      "    freq = freq + [c]\n",
      "print freq\n",
      "\n",
      "# f = csv.writer(open(\"freq_count_al_conc_caut1.csv\", 'w'))\n",
      "# for count in freq:\n",
      "#     f.writerow([count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting frequency of words - panic, crazy in a tweet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq = []\n",
      "for i in target_tweets:\n",
      "    c= (i.count(\"panic\")+i.count(\"crazy\")+i.count(\"funny\")+i.count(\"lol\")+i.count(\"fear\")) \n",
      "    freq = freq + [c]\n",
      "print freq\n",
      "\n",
      "# f = csv.writer(open(\"freq_count_panic_crazy_lol1.csv\", 'w'))\n",
      "# for count in freq:\n",
      "#     f.writerow([count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting frequency of words - Obama, government in a tweet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq = []\n",
      "for i in target_tweets:\n",
      "    c= (i.count(\"obama\")+i.count(\"Obama\")+i.count(\"government\")+i.count(\"Government\")) \n",
      "    freq = freq + [c]\n",
      "print (freq)\n",
      "\n",
      "# f = csv.writer(open(\"freq_count_obama_gov1.csv\", 'wb'))\n",
      "# for count in freq:\n",
      "#     f.writerow([count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting frequency of words - stop, ban in a tweet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq = []\n",
      "for i in target_tweets:\n",
      "    c= (i.count(\"stop\")+i.count(\"Stop\")+i.count(\"ban\")+i.count(\"Ban\")+i.count(\"bans\")) \n",
      "    freq = freq + [c]\n",
      "print (freq)\n",
      "\n",
      "# f = csv.writer(open(\"freq_count_stop_ban1.csv\", 'wb'))\n",
      "# for count in freq:\n",
      "#     f.writerow([count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 0]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lookup names of the Twitter users corresponding to the User ID"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "follower_names = []\n",
      "for i in followers:\n",
      "    usr = twitter_api.users.lookup(user_id =i)\n",
      "    follower_names.append(usr[0]['name'])\n",
      "print follower_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Katherine Bernholz ', u'Clayton T Hoskinson', u'issam turki ']\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Fetch Census Name Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()\n",
      "print 'male name sample:', list(male_names)[:5]\n",
      "print 'female name sample:', list(female_names)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "male name sample: [u'trenton', u'darrin', u'emile', u'jason', u'ron']\n",
        "female name sample: [u'fawn', u'kymberly', u'augustina', u'evalyn', u'chieko']\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classify name according to gender and remove ambiguous names"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_names = []\n",
      "for i in follower_names:\n",
      "    ss = i.lower()\n",
      "    a = ss.split()[0]\n",
      "#     print (a)\n",
      "    first_names.append(a)\n",
      "# print first_names\n",
      "\n",
      "g_list = []\n",
      "\n",
      "for a in first_names:  # 1=male, 2=female\n",
      "    if a in male_names:\n",
      "        gender = 1\n",
      "        g_list.append(gender)\n",
      "    elif a in female_names:\n",
      "        gender = 2\n",
      "        g_list.append(gender)\n",
      "    else:\n",
      "        gender = 0\n",
      "        g_list.append(gender)\n",
      "# print g_list\n",
      "\n",
      "g_list = [x for x in g_list if x!=0.0]\n",
      "print g_list\n",
      "\n",
      "# f = csv.writer(open(\"gender_list2.csv\", 'wb'))\n",
      "# for count in g_list:\n",
      "#     f.writerow([count])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[2, 1]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Once the dataset is harvested it is combined into a dataset consisting of 2000 instances. The process of classifying the data using Logistic Regression and Naive Bayes classification is performed in R and the corresponding code files are provided in the repository."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}